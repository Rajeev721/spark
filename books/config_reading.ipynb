{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "# Creating configParser Object\n",
    "\n",
    "cp = configparser.ConfigParser()\n",
    "\n",
    "# Reading the config files\n",
    "\n",
    "cp.read('D:\\Learning\\config_file_1.ini')\n",
    "\n",
    "#Assigning the parameter values\n",
    "\n",
    "path = cp.get('Paths', 'path')\n",
    "source_file1 = cp.get('Paths', 'source_file1')\n",
    "source_file2 = cp.get('Paths', 'source_file2')\n",
    "Target_file = cp.get('Paths', 'target_file')\n",
    "join_type = cp.get('DB','type_of_join')\n",
    "join_column = cp.get('DB','join_column').strip(\" \")\n",
    "database = cp.get('DB','Database').strip(\" \")\n",
    "target_table = cp.get('DB','target_table').strip(\" \")\n",
    "user_name = cp.get('DB','user_name').strip(\" \")\n",
    "password = cp.get('DB','password').strip(\" \")\n",
    "driver = cp.get('DB', 'driver').strip(\" \")\n",
    "connection_string = cp.get('DB','connection_string').strip(\" \")\n",
    "source_file1_path = os.path.join(path, source_file1)\n",
    "source_file2_path = os.path.join(path, source_file2)\n",
    "Target_file2_path = os.path.join(path, Target_file)\n",
    "df_write = cp.get('DB','df_write')\n",
    "config_table = cp.get('DB','config_table')\n",
    "server_name = cp.get('DB','server')\n",
    "sqldriver = cp.get('DB','sqldriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Config file checkup\").config(\"spark.driver.extraClassPath\",\"D:\\spark\\spark-3.1.2-bin-hadoop2.7\\jars\\mssql-jdbc-9.2.1.jre11.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df1 = spark.read.csv(source_file1_path,header=True,inferSchema=True)\n",
    "source_df2 = spark.read.csv(source_file2_path,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'source_df1.join(source_df2,'+'source_df1.'+ join_column + \" == source_df2.\"+ join_column+ \",\" +\"'\"+ join_type +\"')\"\n",
    "# a = \"source_df1.join(source_df2,source_df1.{0} == source_df2.{0},'{1}')\".format(join_column,join_type)\n",
    "\n",
    "b = 'join_df.write.mode(\"append\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + target_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "# b = 'join_df.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"{0}\").option(\"dbtable\",\"{1}\").option(\"user\",\"{2}\").option(\"password\",\"{3}\").option(\"driver\",\"{4}\").save()'.format(connection_string,target_table,user_name,password,driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = eval(a).select(source_df1.ID,source_df1.Sepal_Length,source_df1.Sepal_Width,source_df2.Petal_Length,source_df2.Petal_Width,source_df2.Species)\n",
    "\n",
    "#a = join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\")\n",
    "#a.show()\n",
    "print(join_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\").write.mode(\"append\").parquet(\"D:\\Learning\\sample_file1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(b)\n",
    "\n",
    "# Following parameter is stored in config file to execute that we are using exec\n",
    "#df_write = 'join_df.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + target_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "#\n",
    "\n",
    "exec(df_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Uploading config details into Table'''\n",
    "df_config_load = spark.read.csv('D:\\Learning\\config_file_load.csv',header=True)\n",
    "df_config_load1 = df_config_load.select(col(\"ID_NUM\").cast('int'),\"SOURCE_PATH\",\"SOURCE_FILE_1\",\"SOURCE_FILE_2\",\"TARGET_FILE\",\"DATABASE_N\",\"CONNECTION_STRING\",\"DRIVER\",\"SOURCE_TABLES\",\"TARGET_TABLE\",\"TYPE_OF_JOIN\",\"JOIN_COLUMN\",\"USER_NAME_V\",\"PASSWORD_V\")\n",
    "df_config_load.printSchema()\n",
    "\n",
    "config_upload = 'df_config_load1.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + config_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "eval(config_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Reading Config details from Table '''\n",
    "config_load = 'spark.read.format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + config_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").load()'\n",
    "config_load_df = eval(config_load)\n",
    "config_load_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from config file\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Config Table\").config(\"spark.driver.extraClassPath\",\"D:\\spark\\spark-3.1.2-bin-hadoop2.7\\jars\\mssql-jdbc-9.2.1.jre11.jar\").getOrCreate()\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "conn = pyodbc.connect('DRIVER='+sqldriver+';SERVER='+server_name+';PORT=1433;DATABASE='+database+';UID='+user_name+';PWD='+ password)\n",
    "\n",
    "#'pyodbc.connect(\"DRIVER=\"{0}\";SERVER=\"{1}\";PORT=1433;DATABASE=\"{2}\";UID=\"{3}\";PWD=\"{4})'.format(sqldriver,server_name,database,user_name,password)\n",
    "\n",
    "\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('select * from SalesLT.config_table where ID_NUM = 1')\n",
    "    row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_NUM = row[0]\n",
    "SOURCE_PATH = row[1]\n",
    "SOURCE_FILE_1 = row[2]\n",
    "SOURCE_FILE_2 = row[3]\n",
    "TARGET_FILE = row[4]\n",
    "DATABASE_N = row[5]\n",
    "CONNECTION_STRING = row[6]\n",
    "DRIVER = row[7]\n",
    "SOURCE_TABLES = row[8]\n",
    "TARGET_TABLE = row[9]\n",
    "TYPE_OF_JOIN = row[10]\n",
    "JOIN_COLUMN = row[11]\n",
    "USER_NAME_V = row[12]\n",
    "PASSWORD_V = row[13]\n",
    "\n",
    "print(ID_NUM)\n",
    "print(SOURCE_PATH)\n",
    "print(SOURCE_FILE_1)\n",
    "print(SOURCE_FILE_2)\n",
    "print(TARGET_FILE)\n",
    "print(DATABASE_N)\n",
    "print(CONNECTION_STRING)\n",
    "print(DRIVER)\n",
    "print(SOURCE_TABLES)\n",
    "print(TARGET_TABLE)\n",
    "print(TYPE_OF_JOIN)\n",
    "print(JOIN_COLUMN)\n",
    "print(USER_NAME_V)\n",
    "print(PASSWORD_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df1 = spark.read.csv(source_file1_path,header=True,inferSchema=True)\n",
    "source_df2 = spark.read.csv(source_file2_path,header=True,inferSchema=True)\n",
    "\n",
    "a = 'source_df1.join(source_df2,'+'source_df1.'+ JOIN_COLUMN + \" == source_df2.\"+ JOIN_COLUMN+ \",\" +\"'\"+ TYPE_OF_JOIN +\"')\"\n",
    "b = 'join_df.write.mode(\"append\").format(\"jdbc\").option(\"url\",\"' + CONNECTION_STRING + '\").option(\"dbtable\",\"' + TARGET_TABLE + '\").option(\"user\",\"' + USER_NAME_V +'\").option(\"password\",\"' + PASSWORD_V + '\").option(\"driver\",\"' + DRIVER + '\").save()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = eval(a).select(source_df1.ID,source_df1.Sepal_Length,source_df1.Sepal_Width,source_df2.Petal_Length,source_df2.Petal_Width,source_df2.Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\").write.mode(\"append\").parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet = spark.read.parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
